{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concentration Gradient Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from environments import BoxEnvironment1 as env\n",
    "from environment_utils import Box, Circle2D\n",
    "from agents import SACAgent\n",
    "from agent_utils import update_target_agent, ReplayBuffer\n",
    "from log_utils import RLLogger\n",
    "from plot_utils import RLPlotter, make_animation\n",
    "\n",
    "device = tr.device('cuda' if tr.cuda.is_available() else 'cpu')\n",
    "tr.autograd.set_detect_anomaly(True)\n",
    "tr.set_default_tensor_type(tr.FloatTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Training -------------------\n",
    "    # Memory\n",
    "memory_size = 4096\n",
    "memory_batch_size = 512\n",
    "    # Duration of training\n",
    "runs = 1\n",
    "n_episodes = 50\n",
    "n_steps = 256\n",
    "    # Training parameters\n",
    "agent_batch_size = 256\n",
    "learning_rate_actor = 0.001\n",
    "learning_rate_critic = 0.001\n",
    "milestones = np.arange(0, n_episodes, n_episodes//3)\n",
    "learing_rate_decay = 0.8\n",
    "\n",
    "entropy_coef = 0.0002 \n",
    "entropy_coef_decay = 1\n",
    "    # Bellman equation\n",
    "future_discount = 0.99\n",
    "    # Update Target Model\n",
    "target_model_update = 1\n",
    "polyak_tau = 0.995\n",
    "    # Loss Function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# ---------------- Environment  ----------------\n",
    "    # Environment box size\n",
    "env_width = 2\n",
    "env_height = 2\n",
    "space = Box(env_width, env_height)\n",
    "    # Goal box size and center\n",
    "\n",
    "goal_radius = 0.1\n",
    "    # Time step size\n",
    "dt = 0.06\n",
    "    # Noise\n",
    "noise_characteristic_length = 2\n",
    "    # Maximum of potential\n",
    "c0 = 0.5\n",
    "\n",
    "# ---------------- Agent ----------------------\n",
    "state_dim = 4\n",
    "hidden_dims = [32,32]\n",
    "act_dim = 1\n",
    "act_positive = True\n",
    "act_scaling = 2*np.pi\n",
    "\n",
    "# ---------------- Other ----------------------\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "plt.rcParams.update({'figure.dpi': 150})\n",
    "total_time = []\n",
    "update_state_time = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = env(space)\n",
    "memory = ReplayBuffer(state_dim, act_dim, memory_size, agent_batch_size)\n",
    "agent = SACAgent(state_dim, act_dim, hidden_dims, act_scaling, act_positive).float().to(device)\n",
    "target_agent = SACAgent(state_dim, act_dim, hidden_dims, act_scaling, act_positive).float().to(device)\n",
    "\n",
    "logger = RLLogger()\n",
    "plotter = RLPlotter(logger, 'logs')\n",
    "testLogger = RLLogger()\n",
    "testPlotter = RLPlotter(testLogger, 'test_logs', test=True)\n",
    "\n",
    "agent.actor_optimizer = tr.optim.Adam(agent.actor.parameters(), lr=learning_rate_actor)\n",
    "agent.critic1_optimizer = tr.optim.Adam(agent.critic1.parameters(), lr=learning_rate_critic)\n",
    "agent.critic2_optimizer = tr.optim.Adam(agent.critic2.parameters(), lr=learning_rate_critic)\n",
    "\n",
    "scheduler_actor = MultiStepLR(agent.actor_optimizer, milestones=milestones, gamma=learing_rate_decay)\n",
    "scheduler_critic1 = MultiStepLR(agent.critic1_optimizer, milestones=milestones, gamma=learing_rate_decay)\n",
    "scheduler_critic2 = MultiStepLR(agent.critic2_optimizer, milestones=milestones, gamma=learing_rate_decay)\n",
    "\n",
    "for p in target_agent.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, target_agent, memory_batch):\n",
    "    agent.critic1_optimizer.zero_grad()\n",
    "    agent.critic2_optimizer.zero_grad()\n",
    "\n",
    "    state_now = memory_batch['state_now'].reshape(-1, state_dim)\n",
    "    state_next = memory_batch['state_next'].reshape(-1, state_dim)\n",
    "    action_now = memory_batch['action_now'].reshape(-1, act_dim)\n",
    "    reward = memory_batch['reward'].reshape(-1)\n",
    "    done = memory_batch['done'].reshape(-1)\n",
    "    \n",
    "    # Compute Prediction\n",
    "    Q1_now_critic = agent.critic1(state_now, action_now)\n",
    "    Q2_now_critic = agent.critic2(state_now, action_now)\n",
    "\n",
    "    # Compute Target\n",
    "    with tr.no_grad():        \n",
    "        action_next_critic, log_prob_next_critic = agent.actor(state_next)\n",
    "        \n",
    "        Q1_next_critic = target_agent.critic1(state_next, action_next_critic)\n",
    "        Q2_next_critic = target_agent.critic2(state_next, action_next_critic)\n",
    "        Q_next_critic = tr.min(Q1_next_critic, Q2_next_critic)\n",
    "        target_critic = reward + future_discount*(Q_next_critic - entropy_coef*log_prob_next_critic)\n",
    "    # Compute Loss\n",
    "    loss_critic = loss_function(Q1_now_critic, target_critic) + loss_function(Q2_now_critic, target_critic)\n",
    "    \n",
    "    # Update\n",
    "    loss_critic.backward()\n",
    "    agent.critic1_optimizer.step()\n",
    "    agent.critic2_optimizer.step()\n",
    "    \n",
    "    agent.actor_optimizer.zero_grad()\n",
    "    for p in agent.critic1.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in agent.critic2.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    action_now_actor, log_prob_now_actor = agent.actor(state_now)\n",
    "    Q1_now_actor = agent.critic1(state_now, action_now_actor)\n",
    "    Q2_now_actor = agent.critic2(state_now, action_now_actor)\n",
    "    Q_now_actor = tr.min(Q1_now_actor, Q2_now_actor)\n",
    "    loss_actor = (entropy_coef*log_prob_now_actor - Q_now_actor).mean()\n",
    "    loss_actor.backward()\n",
    "    agent.actor_optimizer.step()\n",
    "\n",
    "    for p in agent.critic1.parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in agent.critic2.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return loss_critic, loss_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode():\n",
    "    # Initialize Goal at Random Location\n",
    "    # sample = space.sample()\n",
    "    sample = np.array([0.5,0])\n",
    "    goal_center = np.tile(sample,(agent_batch_size,1))\n",
    "    goal = Circle2D(goal_radius, goal_center)\n",
    "\n",
    "    environment.init_env(agent_batch_size, state_dim, goal, c0, random_start = False)\n",
    "    plotter.update_goal(goal)\n",
    "    goal_bool = False\n",
    "    for current_step in range(n_steps):\n",
    "        # Log state\n",
    "        logger.save_state(environment.state)\n",
    "        if current_step%target_model_update == 0 and current_step > memory_size:\n",
    "            update_target_agent(agent, target_agent, polyak_tau)\n",
    "        # Beginning state\n",
    "        state_now = environment.state.copy()\n",
    "        # Action\n",
    "        if memory.size < memory_batch_size:\n",
    "            action_now = 2*tr.pi*tr.rand(agent_batch_size, act_dim, device=device, dtype=tr.float)\n",
    "        else:\n",
    "            action_now, _ = agent.actor(tr.as_tensor(environment.state, device=device, dtype=tr.float))\n",
    "        # Next state\n",
    "        reward = environment.step(action_now.detach().cpu().numpy(), c0, dt, noise_characteristic_length)\n",
    "        state_next = environment.state.copy()\n",
    "        # Done\n",
    "        done = environment.goal_check()\n",
    "        # Log action and reward\n",
    "        logger.save_action(action_now.detach().cpu().numpy())\n",
    "        logger.save_reward(reward)\n",
    "\n",
    "        loss = 0\n",
    "        # Sample from memory\n",
    "        if memory.size >= memory_batch_size:\n",
    "            \n",
    "            memory_batch = memory.sample_batch(memory_batch_size)\n",
    "\n",
    "            # Update Agent\n",
    "            loss_critic, loss_actor = update(agent, target_agent, memory_batch)\n",
    "            loss_critic, loss_actor = loss_critic.item(), loss_actor.item()\n",
    "            logger.save_loss_critic(loss_critic)\n",
    "            logger.save_loss_actor(loss_actor)\n",
    "        \n",
    "        # Store in memory\n",
    "        memory.store(state_now, action_now, reward, state_next, loss, done)\n",
    "        \n",
    "        if max(environment.goal_check()):\n",
    "            goal_bool = True\n",
    "\n",
    "\n",
    "    return current_step, goal_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_episode():\n",
    "    # Initialize Goal at Random Location\n",
    "    # sample = space.sample()\n",
    "    sample = np.array([0.5,0])\n",
    "    goal_center = np.tile(sample,(1,1))\n",
    "    goal = Circle2D(goal_radius, goal_center)\n",
    "\n",
    "    environment.init_env(1, state_dim, goal, c0, random_start = False)\n",
    "    testPlotter.update_goal(goal)\n",
    "    testLogger.save_state(environment.state)\n",
    "    for current_step in range(n_steps):\n",
    "      \n",
    "        # Action\n",
    "        action_now = agent.act(tr.as_tensor(environment.state, device=device, dtype=tr.float), deterministic=True)\n",
    "        environment.step(action_now, c0, dt, noise_characteristic_length, test = True)\n",
    "\n",
    "        # Log Action and State\n",
    "        testLogger.save_action(action_now)\n",
    "        testLogger.save_state(environment.state)\n",
    "            \n",
    "    return current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0  finished!\n",
      "Goal reached!\n",
      "Episode 1  finished!\n",
      "Goal reached!\n",
      "Episode 2  finished!\n",
      "Goal reached!\n",
      "Episode 3  finished!\n",
      "Goal reached!\n",
      "Episode 4  finished!\n",
      "Goal reached!\n",
      "Episode 5  finished!\n",
      "Goal reached!\n",
      "Episode 6  finished!\n",
      "Goal reached!\n",
      "Episode 7  finished!\n",
      "Goal reached!\n",
      "Episode 8  finished!\n",
      "Goal reached!\n",
      "Episode 9  finished!\n",
      "Episode 10  finished!\n",
      "Goal reached!\n",
      "Episode 11  finished!\n",
      "Goal reached!\n",
      "Episode 12  finished!\n",
      "Goal reached!\n",
      "Episode 13  finished!\n",
      "Goal reached!\n",
      "Episode 14  finished!\n",
      "Goal reached!\n",
      "Episode 15  finished!\n",
      "Goal reached!\n",
      "Episode 16  finished!\n",
      "Goal reached!\n",
      "Episode 17  finished!\n",
      "Goal reached!\n",
      "Episode 18  finished!\n",
      "Goal reached!\n",
      "Episode 19  finished!\n",
      "Goal reached!\n",
      "Episode 20  finished!\n",
      "Goal reached!\n",
      "Episode 21  finished!\n",
      "Goal reached!\n",
      "Episode 22  finished!\n",
      "Goal reached!\n",
      "Episode 23  finished!\n",
      "Goal reached!\n",
      "Episode 24  finished!\n",
      "Goal reached!\n",
      "Episode 25  finished!\n",
      "Goal reached!\n",
      "Episode 26  finished!\n",
      "Goal reached!\n",
      "Episode 27  finished!\n",
      "Goal reached!\n",
      "Episode 28  finished!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m plotter\u001b[39m.\u001b[39mclear_plots(\u001b[39m'\u001b[39m\u001b[39mlogs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m testPlotter\u001b[39m.\u001b[39mclear_plots(\u001b[39m'\u001b[39m\u001b[39mtest_logs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m simulation()\n",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m, in \u001b[0;36msimulation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m update_target_agent(agent, target_agent)\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m ep \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_episodes):\n\u001b[0;32m      4\u001b[0m     \u001b[39m# if ep%(n_episodes//10) == 0:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39m#     entropy_coef = entropy_coef * entropy_coef_decay\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     episode_steps, goal_bool \u001b[39m=\u001b[39m episode()\n\u001b[0;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m goal_bool:\n\u001b[0;32m      8\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mGoal reached!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 39\u001b[0m, in \u001b[0;36mepisode\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m memory_batch \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39msample_batch(memory_batch_size)\n\u001b[0;32m     38\u001b[0m \u001b[39m# Update Agent\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m loss_critic, loss_actor \u001b[39m=\u001b[39m update(agent, target_agent, memory_batch)\n\u001b[0;32m     40\u001b[0m loss_critic, loss_actor \u001b[39m=\u001b[39m loss_critic\u001b[39m.\u001b[39mitem(), loss_actor\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     41\u001b[0m logger\u001b[39m.\u001b[39msave_loss_critic(loss_critic)\n",
      "Cell \u001b[1;32mIn[20], line 42\u001b[0m, in \u001b[0;36mupdate\u001b[1;34m(agent, target_agent, memory_batch)\u001b[0m\n\u001b[0;32m     40\u001b[0m Q_now_actor \u001b[39m=\u001b[39m tr\u001b[39m.\u001b[39mmin(Q1_now_actor, Q2_now_actor)\n\u001b[0;32m     41\u001b[0m loss_actor \u001b[39m=\u001b[39m (entropy_coef\u001b[39m*\u001b[39mlog_prob_now_actor \u001b[39m-\u001b[39m Q_now_actor)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m---> 42\u001b[0m loss_actor\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     43\u001b[0m agent\u001b[39m.\u001b[39mactor_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m agent\u001b[39m.\u001b[39mcritic1\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[1;32mc:\\Users\\631lh\\anaconda3\\envs\\RL\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\631lh\\anaconda3\\envs\\RL\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def simulation():\n",
    "    update_target_agent(agent, target_agent)\n",
    "    for ep in range(n_episodes):\n",
    "        # if ep%(n_episodes//10) == 0:\n",
    "        #     entropy_coef = entropy_coef * entropy_coef_decay\n",
    "        episode_steps, goal_bool = episode()\n",
    "        if goal_bool:\n",
    "            print('Goal reached!')\n",
    "            global entropy_coef\n",
    "            entropy_coef = entropy_coef * entropy_coef_decay\n",
    "            # print(entropy_coef)\n",
    "\n",
    "        logger.save_episode(episode_steps)\n",
    "        plotter.plot_last_episode()\n",
    "        \n",
    "\n",
    "        test_episode_steps = test_episode()\n",
    "        testLogger.save_episode(test_episode_steps)\n",
    "        testPlotter.plot_last_episode()        \n",
    "        print('Episode', ep,' finished!')\n",
    "        if memory.size > memory_batch_size:\n",
    "            scheduler_actor.step()\n",
    "            scheduler_critic1.step()\n",
    "            scheduler_critic2.step()\n",
    "        \n",
    "plotter.clear_plots('logs')\n",
    "testPlotter.clear_plots('test_logs')\n",
    "\n",
    "simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(logger.rewards))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video logs/episode_paths_animation.mp4.\n",
      "Moviepy - Writing video logs/episode_paths_animation.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready logs/episode_paths_animation.mp4\n",
      "Moviepy - Building video test_logs/episode_paths_animation.mp4.\n",
      "Moviepy - Writing video test_logs/episode_paths_animation.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test_logs/episode_paths_animation.mp4\n"
     ]
    }
   ],
   "source": [
    "make_animation('logs/episode_paths',3)\n",
    "make_animation('test_logs/episode_paths',3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
